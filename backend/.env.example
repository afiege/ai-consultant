# Database Configuration
DATABASE_URL=sqlite:///./database/ai_consultant.db

# CORS Configuration
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# File Upload Settings
UPLOAD_DIR=./uploads
MAX_FILE_SIZE=10485760

# LLM Configuration (LiteLLM - OpenAI API compatible)
# Model format: provider/model-name
# Examples:
#   mistral/mistral-small-latest (default)
#   openai/gpt-4
#   openai/gpt-3.5-turbo
#   anthropic/claude-3-sonnet-20240229
#   ollama/llama2 (for local models)
LLM_MODEL=mistral/mistral-small-latest

# Optional: Custom API base URL for OpenAI-compatible endpoints
# Leave empty to use provider's default API
# Examples:
#   http://localhost:11434/v1 (Ollama)
#   http://localhost:1234/v1 (LM Studio)
#   http://localhost:8000/v1 (vLLM, text-generation-inference)
#   https://your-endpoint.openai.azure.com (Azure OpenAI)
#
# Academic/Research APIs (OpenAI-compatible):
#   https://llm.scads.ai/v1 (ScaDS.AI - TU Dresden, request key: llm.scads.ai@tu-dresden.de)
#   https://chat-ai.academiccloud.de/v1 (AcademicCloud SAIA)
#
# For ScaDS.AI, use model aliases for automatic fallback:
#   openai/alias-ha (high availability), openai/alias-code, openai/alias-reasoning
LLM_API_BASE=

# API Keys
# Note: API keys are NOT stored on the server - they are entered by users
# in the app at runtime and passed with each request.
# These environment variables are optional fallbacks for development/testing only.
MISTRAL_API_KEY=
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# Server Configuration
HOST=0.0.0.0
PORT=8000

# Development Settings (set to False in production)
DEBUG=False

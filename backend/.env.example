# Database Configuration
DATABASE_URL=sqlite:///./database/ai_consultant.db

# API Keys Encryption
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
ENCRYPTION_KEY=your-fernet-encryption-key-here

# CORS Configuration
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# File Upload Settings
UPLOAD_DIR=./uploads
MAX_FILE_SIZE=10485760

# LLM Configuration (LiteLLM - OpenAI API compatible)
# Model format: provider/model-name
# Examples:
#   mistral/mistral-small-latest (default)
#   openai/gpt-4
#   openai/gpt-3.5-turbo
#   anthropic/claude-3-sonnet-20240229
#   ollama/llama2 (for local models)
LLM_MODEL=mistral/mistral-small-latest

# Optional: Custom API base URL for OpenAI-compatible endpoints
# Leave empty to use provider's default API
# Examples:
#   http://localhost:11434/v1 (Ollama)
#   http://localhost:1234/v1 (LM Studio)
#   http://localhost:8000/v1 (vLLM, text-generation-inference)
#   https://your-endpoint.openai.azure.com (Azure OpenAI)
LLM_API_BASE=

# API Keys - Set the key for your chosen provider
# LiteLLM automatically reads these environment variables
MISTRAL_API_KEY=your-mistral-api-key-here
OPENAI_API_KEY=your-openai-api-key-here
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Server Configuration
HOST=0.0.0.0
PORT=8000

# Development Settings (set to False in production)
DEBUG=False
